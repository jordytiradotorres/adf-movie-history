# ADF Movie History ðŸŽ¬

Una soluciÃ³n **Enterprise** de procesamiento y transformaciÃ³n de datos de pelÃ­culas utilizando **Azure Data Factory (ADF)** que implementa una arquitectura moderna de data ingestion y orquestaciÃ³n en capas (Bronze â†’ Silver â†’ Gold) integrada con **Databricks** para anÃ¡lisis avanzados.

## ðŸ“‹ Tabla de Contenidos

- [DescripciÃ³n General](#descripciÃ³n-general)
- [CaracterÃ­sticas](#caracterÃ­sticas)
- [TecnologÃ­as](#tecnologÃ­as)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Requisitos Previos](#requisitos-previos)
- [InstalaciÃ³n y ConfiguraciÃ³n](#instalaciÃ³n-y-configuraciÃ³n)
- [GuÃ­a de Uso](#guÃ­a-de-uso)
- [Componentes Principales](#componentes-principales)
- [Pipelines](#pipelines)
- [Flujo de Datos](#flujo-de-datos)
- [Monitoreo y Triggers](#monitoreo-y-triggers)

## ðŸŽ¯ DescripciÃ³n General

**ADF Movie History** es una soluciÃ³n completa de ingenierÃ­a de datos que automatiza el procesamiento de informaciÃ³n histÃ³rica sobre pelÃ­culas. Utiliza **Azure Data Factory** como orquestador central para gestionar pipelines ETL, conectÃ¡ndose a **Databricks** para anÃ¡lisis avanzados y almacenÃ¡ndose en **Azure Data Lake Storage (ADLS)**.

El proyecto implementa una arquitectura **medallion** (Bronze-Silver-Gold) asegurando:
- Escalabilidad enterprise
- Calidad de datos garantizada
- AutomatizaciÃ³n completa
- Monitoreo en tiempo real
- Transformaciones reproducibles

## âœ¨ CaracterÃ­sticas

### ðŸ”„ OrquestaciÃ³n Inteligente
- **Pipelines automÃ¡ticos** con Azure Data Factory
- **Triggers programados** para ejecuciÃ³n periÃ³dica
- **Dependencias encadenadas** entre pipelines
- **Manejo robusto de errores** y reintentos automÃ¡ticos
- **Monitoreo en tiempo real** de ejecuciones

### ðŸ“Š Arquitectura Medallion
- **Bronze**: Ingesta de datos raw sin transformar
- **Silver**: Datos limpios, validados y normalizados
- **Gold**: Datos listos para analytics y BI
- **IntegraciÃ³n con Databricks** para procesamiento distribuido
- **Almacenamiento en Azure Data Lake Storage** escalable

### ðŸ”— Conectividad Empresarial
- **Linked Services configurados**: Databricks, ADLS
- **AutenticaciÃ³n segura** con credenciales Azure
- **Conexiones reutilizables** entre pipelines
- **Manejo centralizado** de credenciales
- **Soporte para mÃºltiples fuentes** de datos

### ðŸ“ˆ TransformaciÃ³n de Datos
- **Actividades de copia** (Copy Activity) para ingesta
- **Actividades de Databricks** para procesamiento Spark
- **Actividades SQL** para transformaciones
- **Validaciones integradas** en cada capa
- **AuditorÃ­a y trazabilidad** completa

### ðŸŽ¯ AutomatizaciÃ³n
- **Triggers programados** (daily, hourly, event-based)
- **Notificaciones automÃ¡ticas** de errores
- **Alertas y monitoreo** integrado
- **EjecuciÃ³n paralela** cuando es posible
- **Retry logic** inteligente

## ðŸ”§ TecnologÃ­as

| TecnologÃ­a | DescripciÃ³n | PropÃ³sito |
|-----------|-------------|----------|
| **Azure Data Factory** | Orquestador ETL en la nube | GestiÃ³n de pipelines |
| **Databricks** | Plataforma de analytics | Procesamiento Spark |
| **Azure Data Lake Storage Gen2** | Almacenamiento escalable | Data lake central |
| **Azure Key Vault** | GestiÃ³n de secretos | Credenciales seguras |
| **Apache Spark** | Motor distribuido | TransformaciÃ³n datos |
| **Delta Lake** | Formato transaccional | Almacenamiento fiable |
| **SQL Server** | Base de datos relacional | Metadatos y configuraciÃ³n |
| **Azure Monitor** | Monitoreo y alertas | Observabilidad |
| **GitHub** | Control de versiones | Versionado de cÃ³digo |
| **JSON** | ConfiguraciÃ³n | DefiniciÃ³n de componentes |

## ðŸ“ Estructura del Proyecto

```yaml
adf-movie-history/
â”‚
â”œâ”€â”€ ðŸ“¦ dataset/ # Definiciones de Datasets
â”‚ â””â”€â”€ ds_data_history_bronze.json # Dataset Bronze (datos crudos)
â”‚
â”œâ”€â”€ ðŸ­ factory/ # ConfiguraciÃ³n de Data Factory
â”‚ â””â”€â”€ databricks-course-with-azure-d... # ConfiguraciÃ³n principal ADF
â”‚
â”œâ”€â”€ ðŸ”— linkedService/ # Conexiones a Servicios
â”‚ â”œâ”€â”€ LS_Databricks.json # Linked Service a Databricks
â”‚ â””â”€â”€ ls_movie_history_adls.json # Linked Service a ADLS
â”‚
â”œâ”€â”€ ðŸš€ pipeline/ # Pipelines de OrquestaciÃ³n
â”‚ â”œâ”€â”€ pl_ingest_movie_history_data.json # Pipeline de ingesta
â”‚ â”œâ”€â”€ pl_process_movie_history.json # Pipeline de procesamiento
â”‚ â””â”€â”€ pl_transformation_movie_histor... # Pipeline de transformaciÃ³n
â”‚
â”œâ”€â”€ â²ï¸ trigger/ # Triggers de EjecuciÃ³n
â”‚ â””â”€â”€ tg_process_movie_history.json # Trigger de ejecuciÃ³n programada
â”‚
â”œâ”€â”€ ðŸ“‹ README.md # DocumentaciÃ³n del proyecto
â”œâ”€â”€ ðŸ“„ publish_config.json # ConfiguraciÃ³n de publicaciÃ³n
â””â”€â”€ .gitignore # Archivos ignorados en Git
```

## ðŸ“‹ Requisitos Previos

### SuscripciÃ³n y Permisos Azure
- **SuscripciÃ³n activa** de Azure
- **Permisos de Contributor** en el grupo de recursos
- **Acceso a**:
  - Azure Data Factory
  - Azure Data Lake Storage Gen2
  - Azure Key Vault (para credenciales)
  - Azure Databricks

### Herramientas Necesarias
- **Azure CLI** instalado y configurado
- **Git** para control de versiones
- **Visual Studio Code** (opcional, con extensiÃ³n ARM)
- **Power BI Desktop** (opcional, para visualizaciones)
- **Azure Storage Explorer** (opcional)

### Credenciales y Secretos
- **ConexiÃ³n a Databricks**: Token PAT (Personal Access Token)
- **ConexiÃ³n a ADLS**: Clave de almacenamiento o Managed Identity
- **Service Principal** (recomendado para autenticaciÃ³n)
- **Workspace ID de Databricks**

## ðŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/jordytiradotorres/adf-movie-history.git
cd adf-movie-history
```

### 2. Iniciar SesiÃ³n en Azure

# Iniciar sesiÃ³n
az login

# Seleccionar suscripciÃ³n
az account set --subscription "Tu-ID-SuscripciÃ³n"

# Verificar suscripciÃ³n actual
az account show

### 3. Crear Grupo de Recursos

# Crear grupo de recursos
az group create \
  --name "rg-movie-history" \
  --location "eastus"

### 4. Crear Azure Data Factory

# Crear Data Factory
az datafactory create \
  --resource-group "rg-movie-history" \
  --factory-name "adf-movie-history" \
  --location "eastus"

### 5. Crear Azure Data Lake Storage

# Crear cuenta de almacenamiento
az storage account create \
  --resource-group "rg-movie-history" \
  --name "adlsmoviehistory" \
  --location "eastus" \
  --sku Standard_LRS \
  --kind StorageV2 \
  --hierarchical-namespace true

# Crear contenedores
az storage fs create \
  --account-name "adlsmoviehistory" \
  --name "bronze"

az storage fs create \
  --account-name "adlsmoviehistory" \
  --name "silver"

az storage fs create \
  --account-name "adlsmoviehistory" \
  --name "gold"

### 6. Crear Databricks Workspace

# Crear workspace de Databricks
az databricks workspace create \
  --resource-group "rg-movie-history" \
  --name "dbws-movie-history" \
  --location "eastus" \
  --sku premium

### 7. Importar en Azure Data Factory

# OpciÃ³n 1: Usar Azure Portal
# 1. Ve a Azure Portal â†’ Data Factory
# 2. Abre "Author & Monitor"
# 3. Ve a "Source Control" â†’ Git configuration
# 4. Conecta tu repositorio GitHub
# 5. Importa desde la rama main

# OpciÃ³n 2: Usar Azure CLI (si tus archivos son ARM templates)
# az deployment group create \
#   --resource-group "rg-movie-history" \
#   --template-file "factory/template.json" \
#   --parameters "factory/parameters.json"

### 8. Configurar Linked Services

En Azure Portal â†’ Data Factory â†’ Linked services:

1. LS_Databricks

  - Tipo: Databricks
  - Workspace URL: https://your-instance.cloud.databricks.com
  - Access Token: [Tu PAT token de Databricks]

2. ls_movie_history_adls

  - Tipo: Azure Data Lake Storage Gen2
  - Account name: adlsmoviehistory
  - Authentication method: Account key / Service principal

3. Prueba conexiones antes de continuar

### 9. Crear Datasets

```json
// ds_data_history_bronze.json
{
  "name": "ds_data_history_bronze",
  "type": "AzureBlobFS",
  "linkedServiceName": "ls_movie_history_adls",
  "typeProperties": {
    "folderPath": "bronze",
    "format": "ParquetFormat"
  }
}
```

### 10. Crear Pipelines

- En Azure Data Factory Studio:

1. Ve a "Author" â†’ "Pipelines"
2. Importa los 3 pipelines JSON desde la carpeta /pipeline
3. Configura parÃ¡metros segÃºn tu ambiente

### ðŸŽ® GuÃ­a de Uso

- Flujo de EjecuciÃ³n

```text
1. INGESTA (pl_ingest_movie_history_data)
   â””â”€ Copia datos de fuentes â†’ Bronze Layer
   
2. PROCESAMIENTO (pl_process_movie_history)
   â””â”€ Ejecuta notebooks Databricks
   â””â”€ Transforma Bronze â†’ Silver
   
3. TRANSFORMACIÃ“N (pl_transformation_movie_histor...)
   â””â”€ Crea tablas Gold
   â””â”€ Optimiza para anÃ¡lisis
   
4. TRIGGER (tg_process_movie_history)
   â””â”€ Ejecuta automÃ¡ticamente segÃºn horario
```

### Ejecutar Pipelines Manualmente

# OpciÃ³n 1: Azure Portal
# 1. Ve a Data Factory Studio
# 2. Selecciona el pipeline
# 3. Click "Add trigger" â†’ "Trigger now"

# OpciÃ³n 2: Azure CLI
az datafactory pipeline create-run \
  --resource-group "rg-movie-history" \
  --factory-name "adf-movie-history" \
  --name "pl_ingest_movie_history_data"

### Monitorear Ejecuciones

# Ver estado de ejecuciÃ³n
az datafactory pipeline-run query-by-factory \
  --resource-group "rg-movie-history" \
  --factory-name "adf-movie-history"

# Ver detalles especÃ­ficos
az datafactory pipeline-run show \
  --resource-group "rg-movie-history" \
  --factory-name "adf-movie-history" \
  --run-id "your-run-id"

### ðŸ”§ Componentes Principales

- ðŸ“¦ Datasets
Define la estructura de datos en cada capa:

| Dataset | Capa | Formato | PropÃ³sito |
|---------|------|---------|-----------|
| `ds_data_history_bronze` | Bronze | Parquet/JSON | Almacenar datos crudos |

- ðŸ”— Linked Services
Conexiones reutilizables a servicios externos:

| Servicio | Tipo | AutenticaciÃ³n |
|----------|------|---------------|
| `LS_Databricks` | Databricks | Token PAT |
| `ls_movie_history_adls` | ADLS Gen2 | Key/Service Principal |

# ðŸš€ Pipelines (3 pipelines principales)

## 1. `pl_ingest_movie_history_data`
**PropÃ³sito:** Ingesta de datos desde fuentes

**Actividades:**
- **Copy Activity:** Copia datos â†’ Bronze
- **Validaciones:** Verifica integridad
- **Logging:** Registra metadatos
- **Salida:** Datos raw en `bronze/` container

## 2. `pl_process_movie_history`
**PropÃ³sito:** Procesamiento con Spark en Databricks

**Actividades:**
- **Databricks Notebook Activity**
- Ejecuta notebooks de transformaciÃ³n
- Procesa Bronze â†’ Silver

**Notebooks ejecutados:**
- Limpieza de datos
- NormalizaciÃ³n de formatos
- Enriquecimiento de datos

## 3. `pl_transformation_movie_history`
**PropÃ³sito:** TransformaciÃ³n final para anÃ¡lisis

**Actividades:**
- **SQL Activity:** Crear tablas Gold
- **Spark Activity:** Agregaciones
- Data validation
- **Salida:** Tablas optimizadas en Gold layer

# â²ï¸ Triggers

## `tg_process_movie_history`
**Tipo:** Schedule trigger  
**Frecuencia:** Diaria (configurable)  
**Hora:** 2:00 AM (configurable)  
**Ejecuta:** `pl_ingest_movie_history_data`  
**Reintentos:** AutomÃ¡tico en caso de fallo

### ðŸ“Š Flujo de Datos

```yaml
Fuentes de Datos (APIs, CSV, Databases)
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   INGESTION PIPELINE    â”‚
    â”‚ pl_ingest_movie_...data â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   BRONZE LAYER (ADLS)   â”‚
    â”‚   - Datos raw/crudos    â”‚
    â”‚   - Sin transformar     â”‚
    â”‚   - Formato parquet     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PROCESSING PIPELINE     â”‚
    â”‚ pl_process_movie_...ry  â”‚
    â”‚  (Databricks Spark)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SILVER LAYER (ADLS)   â”‚
    â”‚   - Datos limpios       â”‚
    â”‚   - Validados           â”‚
    â”‚   - Normalizados        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ TRANSFORMATION PIPELINE â”‚
    â”‚ pl_transformation_...ry â”‚
    â”‚   (Agregaciones SQL)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   GOLD LAYER (ADLS)     â”‚
    â”‚  - Datos optimizados    â”‚
    â”‚  - Para anÃ¡lisis/BI     â”‚
    â”‚  - Tablas indexadas     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ANALYSIS & BI          â”‚
    â”‚  Power BI, Databricks   â”‚
    â”‚  SQL Server, Dashboards â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### â²ï¸ Monitoreo y Triggers
 Monitoreo de Pipelines

# Monitorar en tiempo real
# 1. Azure Portal â†’ Data Factory
# 2. Monitor â†’ Pipeline runs
# 3. Ver status, duraciÃ³n, errores

# Alertas automÃ¡ticas
# Configurar en Azure Monitor
# - Fallos de pipeline
# - DuraciÃ³n excesiva
# - Errores de actividad

### Trigger Programado
ConfiguraciÃ³n del trigger tg_process_movie_history:

- Ejecuta pl_ingest_movie_history_data diariamente
- Hora: 2:00 AM UTC (ajustable)
- Reintentos: Hasta 2 intentos en caso de fallo
- Timeout: 48 horas

### Para modificar trigger:

# Ver configuraciÃ³n actual
az datafactory trigger show \
  --resource-group "rg-movie-history" \
  --factory-name "adf-movie-history" \
  --trigger-name "tg_process_movie_history"

### ðŸ“š ConfiguraciÃ³n de Variables de Entorno
Crea un archivo .env (no commitear):

AZURE_SUBSCRIPTION_ID=xxx
AZURE_RESOURCE_GROUP=rg-movie-history
AZURE_DATA_FACTORY=adf-movie-history
DATABRICKS_WORKSPACE_URL=https://xxx.cloud.databricks.com
DATABRICKS_TOKEN=xxx
ADLS_ACCOUNT_NAME=adlsmoviehistory
ADLS_CONTAINER_NAME=bronze

### ðŸ”’ Seguridad
Mejores PrÃ¡cticas Implementadas

âœ… Credenciales en Azure Key Vault
âœ… Managed Identity para autenticaciÃ³n
âœ… EncriptaciÃ³n en trÃ¡nsito (HTTPS/TLS)
âœ… EncriptaciÃ³n en reposo (Storage encryption)
âœ… Control de acceso basado en roles (RBAC)
âœ… AuditorÃ­a y logging habilitados

### Configurar Key Vault

# Crear Key Vault
az keyvault create \
  --resource-group "rg-movie-history" \
  --name "kv-movie-history"

# Agregar secreto
az keyvault secret set \
  --vault-name "kv-movie-history" \
  --name "databricks-token" \
  --value "your-token"

# Usar en ADF: @secretResourceName('kv-movie-history', 'databricks-token')

### ðŸ“ˆ Monitoreo y Alertas
Configura alertas en Azure Monitor:

# Crear alerta para fallos de pipeline
az monitor metrics alert create \
  --resource-group "rg-movie-history" \
  --name "adf-pipeline-failure-alert" \
  --scopes "/subscriptions/.../resourceGroups/rg-movie-history/providers/Microsoft.DataFactory/factories/adf-movie-history" \
  --condition "PipelineFailedRuns > 0" \
  --window-size "PT5M" \
  --evaluation-frequency "PT1M"
